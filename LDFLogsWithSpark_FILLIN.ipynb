{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session: LDF Log Analysis with Spark and Dataframes\n",
    "\n",
    "![LDF Logo](http://linkeddatafragments.org/images/logo.svg)\n",
    "\n",
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data Science Lab has a tradition in Semantic Web Research. Linked Data is basically data with a graph layout where edges are stored as triples: subject -predicate-> object\n",
    "These entities are represented as URIs, the objects can also be string literals. Predicates are often taken from public ontologies. This ensures that linked datasets can be integrated without any additional effort and this is what makes this data model so interesting.\n",
    "\n",
    "Linked Data is offered on the web via a SPARQL endpoint which can be queried using the SPARQL language which has a lot of similarities with SQL. \n",
    "\n",
    "The main difference between SPARQL endpoints and Web APIs is that the first allows unrestricted querying. Unfortunately this means that the server can have arbitrarily complex loads. As a result current Semantic Web SPARQL endpoints have an availability which is < 95%.\n",
    "\n",
    "Data Science Lab has come up with a solution to tackle this availability issue: the linked data fragment server only answers very simple triple queries and some statistics about them, while the linked data fragments client does the more complicated job of performing mainly joins. (more info at: http://linkeddatafragments.org/ or play with an online LDF client here: http://client.linkeddatafragments.org/) \n",
    "\n",
    "The LDF concept is supposed to address the lack of high availability of SPARQL endpoints on the web. We tested this claim by putting an <b>LDF server on Amazon EC2</b> and hosting a number of linked open datasets.\n",
    "The server logs were stored and are offered here as a dataset for you to explore. Log analysis is a Big Data use case: logs can be very high in volume but it's not entirely clear up front which information in the logs is valuable to use (schema on write), therefore collecting the logs in for example HDFS is a more sensible approach.\n",
    "\n",
    "* TIP: you will need the Spark programming guide a lot: http://spark.apache.org/docs/latest/programming-guide.html\n",
    "* TIP: the full API documentation: http://spark.apache.org/docs/latest/api/python/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "#necessary to use matplotlib in a notebook\n",
    "%matplotlib inline  \n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (type(sc))\n",
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start exploring. The logs dataset is quite large, since we are working on a single machine (4 threads) it makes sense to only take a subset while writing our code. Take a subset of the data (1000 records) and cache it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logs = <FILL IN>\n",
    "preview = logs.<FILL IN>\n",
    "for line in preview:\n",
    "    print (line)\n",
    "local_sample = logs.<FILL IN>\n",
    "sample = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After triggering execution you can have a look at the SparkUI on ip_virtual_wall:3030 by clicking on a job and next on <b>DAG Visualization</b>, this visualisation might give you more information in case a one of your jobs is running slowly. (note that you can also click on subtasks and on their DAG visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDF logs you downloaded contain the following information (albeit not in this order)\n",
    "1. the client’s ip address;\n",
    "2. the uri requested by the client;\n",
    "3. value of the client’s Accept header;\n",
    "4. value of the client’s Referer header;\n",
    "5. value of the client’s User-Agent header;\n",
    "6. the server’s local time;\n",
    "7. the server’s response size;\n",
    "8. the server’s response cache status; \n",
    "9. the server’s response http status code. \n",
    "\n",
    "More about apache web logs at: https://httpd.apache.org/docs/1.3/logs.html (note that the logs contain more information than the standard web logs)\n",
    "\n",
    "The logs are somewhat structured but we will have to do some preprocessing to have them in a clean format. Let's try to manually extract all these components from a single log line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logline = local_sample[0]\n",
    "print (logline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to identify the different parts of the log line, by simply slicing the log line, this will help you in the next steps where you will try to automate this with regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you get all the individual components of this log string? Write a function to split by space? Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = logline.<FILL IN>\n",
    "for p in parts:\n",
    "    print (p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The safest way to parse a string is to use <b>regular expressions</b>, since they allow for fine-grained data quality control. \n",
    "\n",
    "For a quick intro into regular expressions see: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "For a regex tester: http://pythex.org/\n",
    "\n",
    "Let's start with some basic symbols:\n",
    "\n",
    "<b>^</b>,<b>$</b> are beginning/end of string\n",
    "\n",
    "<b>\\S</b> matches any non-whitespace character. \n",
    "\n",
    "<b>\\w</b> matches a word character while <b>\\W</b> matches any non word character\n",
    "\n",
    "<b>\\d</b> matches a digit\n",
    "\n",
    "<b>.</b> is a wildcard (i.e. matches any character)\n",
    "\n",
    "if you want to match <b> special characters </b> you need to escape them, for example <b>\\\\[</b>\n",
    "\n",
    "<b> Quantities: </b>\n",
    "\n",
    "\\+ 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's\n",
    "\n",
    "\\* 0 or more occurrences of the pattern to its left\n",
    "\n",
    "?  match 0 or 1 occurrences of the pattern to its left \n",
    "\n",
    "<b> Brackets: </b>\n",
    "\n",
    "are  in principle an or operator: <b>[ab]</b> matches a or b\n",
    "\n",
    "curly bracks are used for repeats: <b>\\d{3}</b> corresponds to 3 digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#demo\n",
    "match = re.search(\"^\\S+\", logline)\n",
    "print (match)\n",
    "print (match.group())\n",
    "match = re.search(\"^\\S+ \\S+\", logline)\n",
    "print (match.group())\n",
    "match = re.search(\"^\\S+_\\S+\", logline)\n",
    "print (match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can match multiple patterns by using groups, which are surrounded by curly brackets (), for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match = re.search(\"^(\\S+) (\\S+) (\\S+)\", logline)\n",
    "\n",
    "print (match.group())\n",
    "print (match.group(1))\n",
    "print (match.group(2))\n",
    "print (match.group(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now proceed to make a regex which extracts all the fields from the log string. Then write a function qualitycheck with returns (0,logline) if the line doesn't parse, or (1, Row) if it does (https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "regex = <FILL IN>\n",
    "\n",
    "match = re.search(regex,logline)\n",
    "for i in range(1,12):\n",
    "    print (match.group(i))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "def qualityCheck1(regex, logline):\n",
    "\n",
    "    match = <FILL IN>\n",
    "    \n",
    "    if (match == None):\n",
    "        <FILL IN>\n",
    "    \n",
    "    else:\n",
    "        return (1, Row(\n",
    "        client_ip = match.group(1),\n",
    "        identd = <FILL IN>,\n",
    "        userid = <FILL IN>,\n",
    "        server_local_time = <FILL IN>,\n",
    "        uri_request = <FILL IN>,\n",
    "        server_http_statuscode = <FILL IN>,\n",
    "        server_response_size = <FILL IN>,\n",
    "        client_referer_header = <FILL IN>,\n",
    "        client_user_agent_header = <FILL IN>,\n",
    "        client_accept_header = <FILL IN>,\n",
    "        server_cache_status = <FILL IN>\n",
    "        ))\n",
    "    \n",
    "r = qualityCheck1(regex, logline)    \n",
    "print (r[1].client_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it's time to check whether our server logs comply with the suggested regex pattern, use the correct spark functions to convert the log lines and count the number of lines which have parsed successfully and the number that didn't parse successfully. Next have a look at the ones that didn't parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedLogs = <FILL IN>\n",
    "badLogs = <FILL IN>\n",
    "goodLogs = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Number of times parse successful: \" + str(goodLogs.count()))\n",
    "print (\"Number of times parse not succesful: \" + str(badLogs.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reasonable chance that not all log lines where parsed correctly, so the previous process will be iterative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have found a working regex, rerun the log parsing, the number of successful parses should be 100%. Note that this process might take some time, make sure to monitor the job progress via the <b>Spark UI</b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDF specific analysis\n",
    "\n",
    "NOTE: it's more interesting to take a real random sample for the remainder of the lab, the original sample only contains data of one month, start by taking a 'real_sample' of 1% of the logs\n",
    "\n",
    "#### **1a. How many requests are contained in the sample?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove sample RDD from cache\n",
    "<FILL IN>\n",
    "\n",
    "#NOTE: when applying multiple transformations apply the coding style below)\n",
    "real_sample = (logs\n",
    "               <FILL IN>\n",
    "               .cache()\n",
    "               )\n",
    "\n",
    "print (\"1a) \" + str(real_sample.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1b. How many different months does your sample cover?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1b\n",
    "def extractMonthYear(slt):\n",
    "    start = <FILL IN>\n",
    "    stop = <FILL IN>\n",
    "    return slt[start:stop]\n",
    "\n",
    "months = (real_sample\n",
    "              <FILL IN>\n",
    "              .collect()\n",
    "                  )\n",
    "\n",
    "print (\"1b) \" + str(len(months)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1c. How many requests per month?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requests_per_month = (real_sample\n",
    "              <FILL IN>\n",
    "              .collectAsMap()        \n",
    "                     )\n",
    "print(\"1c) \" + str(requests_per_month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### **1d. Visualize the number of requests per month in the sample **\n",
    "\n",
    "Have a look at the plot and the bar function in matplotlib:\n",
    "\n",
    "* help(plt.plot)\n",
    "\n",
    "* help(plt.bar)\n",
    "\n",
    "<b>Note</b> that a python function consists of a list of obligatory arguments and a number of optional arguments of the type key=value\n",
    "\n",
    "http://matplotlib.org/api/pyplot_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shift the xticks to the center of the bars\n",
    "l = list(range(len(requests_per_month)))  \n",
    "shifted = [x + 0.5 for x in l]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(range(len(requests_per_month)), requests_per_month.values()) \n",
    "plt.xticks(shifted, list(requests_per_month.keys()))\n",
    "plt.title(\"Number of requests per month in a 1% sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge peak at April/2015 is due to the fact that the LDF Server was used in a benchmarking study at a conference which was organised that same month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. How effective has the cache been? **\n",
    "#### **2a. What are the different values for cache status? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache_values = (<FILL IN>)\n",
    "                  \n",
    "dist_values = <FILL IN>                  )\n",
    "print (dist_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2b. What are their frequencies + one more pie chart? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = cache_values.count()\n",
    "\n",
    "cache_value_freqs = (<FILL IN>\n",
    "                        .collectAsMap()\n",
    "                    )\n",
    "\n",
    "print (cache_value_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.pie(list(cache_value_freqs.values()), labels=list(cache_value_freqs.keys()), autopct='%1.1f%%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Estimate the availability of the LDF server based on the number of pingdom requests **\n",
    "\n",
    "Pingdom was used to test the high availability of the server by sending a request every minute, adding up to 393,120 minutes. Since we are taking a 1% sample we can expect the number of pingdom requests to be approximately 3931,2 . How many are in the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = 3931.2\n",
    "freq_ping = <FILL IN>\n",
    "sample_freq = <FILL IN>\n",
    "\n",
    "print (sample_freq / expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 4. What kind of triple patterns were requested?**\n",
    "#### ** 4a. Use the dataframe API for this question, start by selecting the relevant columns (date, ip, response split into s, p, and o )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def parseURIRequest(req):\n",
    "    qm_id = req.find('?')\n",
    "    sub = '*'\n",
    "    pred = '*'\n",
    "    obj = '*'\n",
    "    \n",
    "    if (qm_id < 0):\n",
    "        return sub,pred,obj\n",
    "    else:\n",
    "        req_clean = req[0:req.rfind(\" \")]\n",
    "        \n",
    "        spo = req_clean.split('?')[1]\n",
    "        spo_split = spo.split('&')\n",
    "        \n",
    "        for s in spo_split:\n",
    "            entitity_split = s.split('=')\n",
    "            print (entitity_split[0])\n",
    "            print (entitity_split[1])\n",
    "            if entitity_split[0] == \"subject\":\n",
    "                sub = entitity_split[1]\n",
    "            elif entitity_split[0] == \"predicate\":\n",
    "                pred = entitity_split[1]\n",
    "            else :\n",
    "                obj = entitity_split[1]\n",
    "    \n",
    "        return sub,pred,obj\n",
    "    \n",
    "test = (real_sample\n",
    "              <FILL IN>\n",
    "        )\n",
    "\n",
    "df = test.toDF([\"stime\", \"ip\", \"s\",\"p\",\"o\"])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "pretty_url = udf(<FILL IN>)\n",
    "\n",
    "df_pretty = df.select(pretty_url(df.s), pretty_url(df.p), pretty_url(df.o))\n",
    "\n",
    "df_pretty = (df_pretty\n",
    "             .withColumnRenamed(\"PythonUDF#<lambda>(s)\", \"s\")\n",
    "        .withColumnRenamed(\"PythonUDF#<lambda>(p)\", \"p\")\n",
    "        .withColumnRenamed(\"PythonUDF#<lambda>(o)\", \"o\")\n",
    "             )\n",
    "\n",
    "df_pretty.show()\n",
    "\n",
    "\n",
    "print (\"total records: \" + str(df_pretty.count()))\n",
    "\n",
    "star_star_star = <FILL IN>\n",
    "star_star_notstar = <FILL IN>\n",
    "star_notstar_star = <FILL IN>\n",
    "star_notstar_notstar = <FILL IN>\n",
    "\n",
    "notstar_star_star = <FILL IN>\n",
    "notstar_star_notstar = <FILL IN>\n",
    "notstar_notstar_star = <FILL IN>\n",
    "notstar_notstar_notstar = <FILL IN>\n",
    "\n",
    "print ( \"(*,*,*): \" + str(star_star_star.count()))\n",
    "print ( \"(*,*,o): \" + str(star_star_notstar.count()))\n",
    "print ( \"(*,p,*): \" + str(star_notstar_star.count()))\n",
    "print ( \"(*,p,o): \" + str(star_notstar_notstar.count()))\n",
    "print ( \"(s,*,*): \" + str(notstar_star_star.count()))\n",
    "print ( \"(s,*,o): \" + str(notstar_star_notstar.count()))\n",
    "print ( \"(s,p,*): \" + str(notstar_notstar_star.count()))\n",
    "print ( \"(s,p,o): \" + str(notstar_notstar_notstar.count()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **  5. Is there a trend in the LDF server usage? (number of requests per day) **\n",
    "#### **  5a. Start by mapping every day on an integer number (be pragmatic, it shouldn't be perfect!) **\n",
    "(TIP: Do you remember the purpose of broadcast variables and accumulators?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = {'Jan':3, 'Feb':4, 'Mar':5, 'Apr':6, 'May':7, 'Jun':8, 'Jul':9, 'Aug':10, 'Sep':11, \n",
    "           'Oct':0, 'Nov':1, 'Dec':2}\n",
    "\n",
    "def extractDayMonth(slt):\n",
    "    #keep in mind that the mapping dictionary should be shipped to the workers\n",
    "    day = <FILL IN>\n",
    "    month = <FILL IN>\n",
    "    return (day,month)\n",
    "\n",
    "day_month_to_int = (real_sample\n",
    "              <FILL IN>\n",
    "    \n",
    "            )\n",
    "\n",
    "day_month_frequencies = (<FILL IN>\n",
    "                        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **  5b. Create a bar chart with the number of requests per day **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_day = <FILL IN>\n",
    "last_day = <FILL IN>\n",
    "frequencies = day_month_frequencies.collect()\n",
    "\n",
    "values = [0] * (last_day - first_day +1)\n",
    "keys = range(first_day, last_day +1)\n",
    "\n",
    "for f in frequencies:\n",
    "    values[f[0]-first_day]=f[1]\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(range(len(values)), values) \n",
    "plt.title(\"Number of requests per day in a 1% sample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
